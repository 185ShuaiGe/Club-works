{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三章：线性神经网络\n",
    "## 一.小批量梯度下降(MBGD/SGD)\n",
    "###\n",
    "批量梯度下降：每次更新使用全部数据进行求梯度\n",
    "随机梯度下降：每次更新选择一条数据计算梯度\n",
    "小批量梯度下降：每次更新选择一个小批量(如32，256等)条数据计算梯度\n",
    "### 具体实现\n",
    "损失函数\n",
    "![](https://imgs.xiedaimala.com/6axMoRyLbvXiFnmYj7hAs3DC4G7Embkq/%25E5%25B1%258F%25E5%25B9%2595%25E6%2588%25AA%25E5%259B%25BE%25202025-03-02%2520160703.png)<!-- .element: style=\"height:200px\" --> \n",
    "  \n",
    "梯度更新\n",
    "\n",
    "![](https://imgs.xiedaimala.com/etJbDhAsB4YxS30nC6QW5zRrEce9aaS5/%25E5%25B1%258F%25E5%25B9%2595%25E6%2588%25AA%25E5%259B%25BE%25202025-03-02%2520160842.png)<!-- .element: style=\"height:200px\" -->   \n",
    " \n",
    "### 代码实现\n",
    "\n",
    "```\n",
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"构造一个Gluon数据迭代器\"\"\"\n",
    "    dataset = gluon.data.ArrayDataset(*data_arrays)\n",
    "    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "\n",
    "```  \n",
    "### 优点与缺点\n",
    "比SGD更稳定的收敛，比Batch GD更节省内存\n",
    "\n",
    "## 二.一些特殊的操作\n",
    "### 构建网络\n",
    "nn是神经网络的缩写\n",
    "\n",
    "\n",
    "```\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(1)) \n",
    "```  \n",
    "```\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'),\n",
    "nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "```  \n",
    "### \n",
    "定义损失函数\n",
    "\n",
    "``` \n",
    "loss = gluon.loss.L2Loss()\n",
    "```  \n",
    "定义优化算法\n",
    "\n",
    "```\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})\n",
    "```  \n",
    "### \n",
    "迭代训练网络\n",
    "\n",
    "```[1-3|4-7]  \n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l.mean().asnumpy():f}')\n",
    "```  \n",
    "   \n",
    "## Softmax回归\n",
    "### 计算公式\n",
    "![](https://imgs.xiedaimala.com/wP7q5KwCiad93KsKX60ynnjTorgng9Ow/%25E5%25B1%258F%25E5%25B9%2595%25E6%2588%25AA%25E5%259B%25BE%25202025-03-02%2520212922.png)<!-- .element: style=\"height:200px\" -->    \n",
    "### 定义损失函数\n",
    "想要预测结果正确，无需预测概率等于标签值，只需概率大就行了\n",
    "![](https://imgs.xiedaimala.com/llmMSf5b9ucXwHEte7bDEabccdh7RsIV/%25E5%25B1%258F%25E5%25B9%2595%25E6%2588%25AA%25E5%259B%25BE%25202025-03-02%2520213502.png)<!-- .element: style=\"height:200px\" -->   "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
